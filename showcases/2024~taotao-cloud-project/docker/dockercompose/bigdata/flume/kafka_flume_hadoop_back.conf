#source的名字
agent.sources = kafkaSource
# channels的名字，建议按照type来命名
agent.channels = memoryChannel
# sink的名字，建议按照目标来命名
agent.sinks = hdfsSink
 
# 指定source使用的channel名字
agent.sources.kafkaSource.channels = memoryChannel
# 指定sink需要使用的channel的名字,注意这里是channel
agent.sinks.hdfsSink.channel = memoryChannel

agent.sources.kafkaSource.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.kafkaSource.channels = memoryChannel
agent.sources.kafkaSource.batchSize = 1000
agent.sources.kafkaSource.batchDurationMillis = 2000
agent.sources.kafkaSource.kafka.bootstrap.servers = bigdata-kafka-broker1:9092,bigdata-kafka-broker2:9092,bigdata-kafka-broker3:9092
agent.sources.kafkaSource.kafka.topics = access
agent.sources.kafkaSource.kafka.consumer.group.id = kafka2flume2hdfs
 
agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity=1000
agent.channels.memoryChannel.transactionCapacity=1000

agent.sinks.hdfsSink.type = hdfs
agent.sinks.hdfsSink.channel = memoryChannel
agent.sinks.hdfsSink.hdfs.path = hdfs://bigdata-hadoop-master:9000/user/hive/warehouse/access/${ctime}
agent.sinks.hdfsSink.hdfs.filePrefix = events-
agent.sinks.hdfsSink.hdfs.round = true
agent.sinks.hdfsSink.hdfs.roundValue = 10
agent.sinks.hdfsSink.hdfs.roundUnit = minute
 
agent.sources.kafkaSource.interceptors = i1
agent.sources.kafkaSource.interceptors.i1.type = com.taotao.cloud.flume.interceptor.AccessLogSourceInterceptor$Builder
